{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import codecs\n",
    "from gensim import corpora, models, similarities\n",
    "#定义停用词、标点符号\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "#定义语料\n",
    "jqcomment = pd.read_excel('D:/泰迪杯/c题/附件1/景区评论.xlsx')\n",
    "jqscore = pd.read_excel('D:/泰迪杯/c题/附件2/景区评分.xlsx')\n",
    "jqcomment.index=jqcomment['景区名称']\n",
    "jqscore.index=jqscore['景区名称']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = 'stopwords.txt'\n",
    "stopwords = codecs.open(stop_words,'r',encoding='gb2312').readlines()\n",
    "stopwords = [ w.strip() for w in stopwords ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.000000\n",
       "mean      4.435500\n",
       "std       0.170121\n",
       "min       4.090000\n",
       "25%       4.318750\n",
       "50%       4.415000\n",
       "75%       4.551250\n",
       "max       4.885000\n",
       "Name: 总得分, dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jqscore['总得分'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = jqscore.sort_values(by=\"总得分\" ,ascending=False)\n",
    "#取出前三名\n",
    "a = df[:3]['景区名称'].values\n",
    "# #取出末尾三名\n",
    "b = df[-3:]['景区名称'].values\n",
    "# # #取出中等三名\n",
    "c = df[24:27]['景区名称'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前三景区： ['A39' 'A38' 'A23'] 尾三景区： ['A27' 'A04' 'A11'] 中等景区： ['A28' 'A14' 'A22']\n"
     ]
    }
   ],
   "source": [
    "print(\"前三景区：\",a,\"尾三景区：\",b,\"中等景区：\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = [jqcomment.loc[f'{x}']['评论内容'] for x in a]\n",
    "df2 = [jqcomment.loc[f'{x}']['评论内容'] for x in b]\n",
    "df3 = [jqcomment.loc[f'{x}']['评论内容'] for x in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[str(i) for i in df1[0]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list = x1\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list1 = []\n",
    "for sentence in sentences_list:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list1.append(line_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除标点符号\n",
    "words = []\n",
    "data = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list1:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data = [x for y in data for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def similar(data,word):\n",
    "    sentence_list = []\n",
    "    model = Word2Vec(data, sg=1,  window=5,  min_count=2,  negative=1, sample=0.001, hs=1, workers=4)\n",
    "    return model.wv.similar_by_word(word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不错', 0.1628482583605263),\n",
       " ('公园', 0.15718559500101217),\n",
       " ('动物', 0.15119772700187245),\n",
       " ('好玩', 0.10031742283527327),\n",
       " ('门票', 0.08701794275222671)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df1[0]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-da19a004a57e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimilar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'公园'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-162-34acebcd8315>\u001b[0m in \u001b[0;36msimilar\u001b[1;34m(data, word)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msimilar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msentence_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacnda\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab)\u001b[0m\n\u001b[0;32m    420\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m                 end_alpha=self.min_alpha, compute_loss=self.compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacnda\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_training_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anacnda\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "similar(data,'公园')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=[str(i) for i in df1[1]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list2 = x2\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list2 = []\n",
    "for sentence in sentences_list2:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list2.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data2 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list2:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data2.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data2 = [x for y in data2 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('温泉', 0.31889320984381186),\n",
       " ('不错', 0.1117743382507389),\n",
       " ('阳西', 0.08984714606394557),\n",
       " ('环境', 0.06564127623415904),\n",
       " ('下次', 0.05725088924142857)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df1[1]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('咸水', 0.24582242965698242),\n",
       " ('多种', 0.2407265454530716),\n",
       " ('赞', 0.21242684125900269),\n",
       " ('网上', 0.2021472007036209),\n",
       " ('效果', 0.1997610330581665),\n",
       " ('养生', 0.19799770414829254),\n",
       " ('住', 0.17157791554927826),\n",
       " ('远', 0.171174094080925),\n",
       " ('第一次', 0.15945076942443848),\n",
       " ('小卖部', 0.158113032579422)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(data2,'温泉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3=[str(i) for i in df1[2]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list3 = x3\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list3 = []\n",
    "for sentence in sentences_list3:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list3.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data3 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list3:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data3.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data3 = [x for y in data3 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不错', 0.15013325887769705),\n",
       " ('园林', 0.1484544882859261),\n",
       " ('值得', 0.09782318912419212),\n",
       " ('中山', 0.09208658483300493),\n",
       " ('景色', 0.08292516137942735)]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df1[2]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('风格', 0.3285103142261505),\n",
       " ('特色', 0.3131920099258423),\n",
       " ('苏州园林', 0.2964155375957489),\n",
       " ('中山', 0.28874659538269043),\n",
       " ('江南', 0.26307737827301025),\n",
       " ('精致', 0.2544037103652954),\n",
       " ('岭南', 0.24525584280490875),\n",
       " ('苏杭', 0.24293270707130432),\n",
       " ('古典', 0.24247203767299652),\n",
       " ('环境', 0.23746515810489655)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(data3,'园林')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
