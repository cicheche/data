{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import codecs\n",
    "from gensim import corpora, models, similarities\n",
    "#定义停用词、标点符号\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "#定义语料\n",
    "jqcomment = pd.read_excel('D:/泰迪杯/c题/附件1/景区评论.xlsx')\n",
    "jqscore = pd.read_excel('D:/泰迪杯/c题/附件2/景区评分.xlsx')\n",
    "jqcomment.index=jqcomment['景区名称']\n",
    "jqscore.index=jqscore['景区名称']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50.000000\n",
       "mean      4.435500\n",
       "std       0.170121\n",
       "min       4.090000\n",
       "25%       4.318750\n",
       "50%       4.415000\n",
       "75%       4.551250\n",
       "max       4.885000\n",
       "Name: 总得分, dtype: float64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jqscore['总得分'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = jqscore.sort_values(by=\"总得分\" ,ascending=False)\n",
    "#取出前三名\n",
    "a = df[:3]['景区名称'].values\n",
    "# #取出末尾三名\n",
    "b = df[-3:]['景区名称'].values\n",
    "# # #取出中等三名\n",
    "c = df[24:27]['景区名称'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前三景区： ['A39' 'A38' 'A23'] 尾三景区： ['A27' 'A04' 'A11'] 中等景区： ['A28' 'A14' 'A22']\n"
     ]
    }
   ],
   "source": [
    "print(\"前三景区：\",a,\"尾三景区：\",b,\"中等景区：\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = [jqcomment.loc[f'{x}']['评论内容'] for x in a]\n",
    "df2 = [jqcomment.loc[f'{x}']['评论内容'] for x in b]\n",
    "df3 = [jqcomment.loc[f'{x}']['评论内容'] for x in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=[str(i) for i in df1[0]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list = x1\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list1 = []\n",
    "for sentence in sentences_list:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list1.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list1:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data = [x for y in data for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "def similar(data,word):\n",
    "    sentence_list = []\n",
    "    model = Word2Vec(data, sg=1,  window=5,  min_count=2,  negative=1, sample=0.001, hs=1, workers=4)\n",
    "    return model.wv.similar_by_word(word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不错', 0.1628482583605263),\n",
       " ('公园', 0.15718559500101217),\n",
       " ('动物', 0.15119772700187245),\n",
       " ('好玩', 0.10031742283527327),\n",
       " ('门票', 0.08701794275222671)]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df1[0]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "A39 = similar(data,'公园')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=[str(i) for i in df1[1]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list2 = x2\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list2 = []\n",
    "for sentence in sentences_list2:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list2.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data2 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list2:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data2.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data2 = [x for y in data2 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('温泉', 0.31889320984381186),\n",
       " ('不错', 0.1117743382507389),\n",
       " ('阳西', 0.08984714606394557),\n",
       " ('环境', 0.06564127623415904),\n",
       " ('下次', 0.05725088924142857)]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df1[1]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "A38 = similar(data2,'温泉')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3=[str(i) for i in df1[2]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list3 = x3\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list3 = []\n",
    "for sentence in sentences_list3:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list3.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data3 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list3:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data3.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data3 = [x for y in data3 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不错', 0.15013325887769705),\n",
       " ('园林', 0.1484544882859261),\n",
       " ('值得', 0.09782318912419212),\n",
       " ('中山', 0.09208658483300493),\n",
       " ('景色', 0.08292516137942735)]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df1[2]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "A23 = similar(data3,'园林')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4=[str(i) for i in df2[0]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list4 = x4\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list4 = []\n",
    "for sentence in sentences_list4:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list4.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data4 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list4:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data4.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data4 = [x for y in data4 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不错', 0.16847827593565254),\n",
       " ('取票', 0.12054648879328723),\n",
       " ('沙滩', 0.11959763836031113),\n",
       " ('浪漫', 0.09531010549040045),\n",
       " ('方便', 0.07805628199052435)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df2[0]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "A27 = similar(data4,'不错')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "x5=[str(i) for i in df2[1]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list5 = x5\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list5 = []\n",
    "for sentence in sentences_list5:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list5.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data5 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list5:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data5.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data5 = [x for y in data5 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('好玩', 0.1847092951801462),\n",
       " ('排队', 0.13087006948969884),\n",
       " ('项目', 0.11948650220783064),\n",
       " ('不错', 0.11153852243553027),\n",
       " ('刺激', 0.08423888003027462)]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df2[1]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "A04 = similar(data5,'项目')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "x6=[str(i) for i in df2[2]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list6 = x6\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list6 = []\n",
    "for sentence in sentences_list6:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list6.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data6 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list6:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data6.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data6 = [x for y in data6 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不错', 0.19126274591093967),\n",
       " ('取票', 0.13351384411840944),\n",
       " ('好玩', 0.1095224598492286),\n",
       " ('婚纱照', 0.10673932680878916),\n",
       " ('景色', 0.09958306771964781)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df2[2]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "A11 = similar(data6,'婚纱照')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "x7=[str(i) for i in df3[0]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list7 = x7\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list7 = []\n",
    "for sentence in sentences_list7:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list7.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data7 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list7:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data7.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data7 = [x for y in data7 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('取票', 0.31309130913106814),\n",
       " ('方便', 0.25021909632362893),\n",
       " ('排队', 0.13949817059974154),\n",
       " ('套票', 0.135865808374632),\n",
       " ('不错', 0.13134902317610406)]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df3[0]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "A28 = similar(data7,'取票')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "x8=[str(i) for i in df3[1]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list8 = x8\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list8 = []\n",
    "for sentence in sentences_list8:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list8.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data8 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list8:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data8.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data8 = [x for y in data8 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('地下河', 0.14682455100734448),\n",
       " ('不错', 0.13299972788154987),\n",
       " ('值得', 0.1320649619490162),\n",
       " ('取票', 0.1239445397228167),\n",
       " ('景点', 0.10168484059929694),\n",
       " ('景色', 0.10071598585044347),\n",
       " ('溶洞', 0.09087244229191842),\n",
       " ('坐船', 0.0855735371551612),\n",
       " ('景区', 0.08487683739079084),\n",
       " ('方便', 0.07486164550967837)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df3[1]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "A14 = similar(data8,'地下河')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "x9=[str(i) for i in df3[2]]\n",
    "import re\n",
    "#加载停用词\n",
    "stoplist= [word.strip() for word in open('stopwords.txt',encoding='gb2312').readlines()]\n",
    "# print(stoplist)\n",
    "sentences_list9 = x9\n",
    "# 对句子进行分词\n",
    "def seg_depart(sentence):\n",
    "    # 去掉非汉字字符\n",
    "    sentence = re.sub(r'[^\\u4e00-\\u9fa5]+','',sentence)\n",
    "    sentence_depart = jieba.cut(sentence.strip())\n",
    "    word_list = []\n",
    "    for word in sentence_depart:\n",
    "        if word not in stoplist:\n",
    "            word_list.append(word)\n",
    "    # 如果句子整个被过滤掉了，如：'02-2717:56'被过滤，那就返回[],保持句子的数量不变\n",
    "    return word_list\n",
    "sentence_word_list9 = []\n",
    "for sentence in sentences_list9:\n",
    "    line_seg = seg_depart(sentence)\n",
    "    sentence_word_list9.append(line_seg)\n",
    "    \n",
    "#去除标点符号\n",
    "words = []\n",
    "data9 = []\n",
    "punctuation = [\"，\",\"。\",\"\",\"；\", \"？\"]\n",
    "for word in sentence_word_list9:\n",
    "    if word not in punctuation:          \n",
    "        words.append(word)\n",
    "    data9.append(words)\n",
    "    \n",
    "#数据降维\n",
    "data9 = [x for y in data9 for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('不错', 0.13286666368895353),\n",
       " ('爬山', 0.09413666636152102),\n",
       " ('风景', 0.090062856228488),\n",
       " ('可以', 0.07420288574352965),\n",
       " ('观音', 0.06895261936704826),\n",
       " ('值得', 0.0682241133716976),\n",
       " ('山顶', 0.061044243864440656),\n",
       " ('景色', 0.05245693433797732),\n",
       " ('空气', 0.04949907234200462),\n",
       " ('门票', 0.04744210905142972)]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "# 基于 TF-IDF 算法的关键词抽取\n",
    "tags = jieba.analyse.extract_tags(\",\".join(df3[2]), topK=5,withWeight=True, allowPOS=())\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "A22 = similar(data9,'不错')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前三景区： ['A39' 'A38' 'A23'] 尾三景区： ['A27' 'A04' 'A11'] 中等景区： ['A28' 'A14' 'A22']\n"
     ]
    }
   ],
   "source": [
    "print(\"前三景区：\",a,\"尾三景区：\",b,\"中等景区：\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A39景区公园的相关特点 [('茂名', 0.22755761444568634), ('内容', 0.2224169820547104), ('目的地', 0.22235941886901855), ('四季', 0.2220999151468277), ('要钱', 0.21753837168216705), ('第三次', 0.21502384543418884), ('真不错', 0.2137516289949417), ('入到', 0.20467567443847656), ('越', 0.20305506885051727), ('区有', 0.19691939651966095)]\n",
      "A38景区温泉的相关特点 [('咸水', 0.23790034651756287), ('多种', 0.23033678531646729), ('赞', 0.21837648749351501), ('第一次', 0.20825782418251038), ('养生', 0.20104221999645233), ('好好', 0.17623820900917053), ('环境', 0.17530910670757294), ('住', 0.17511841654777527), ('效果', 0.17418308556079865), ('足', 0.1684282124042511)]\n",
      "A23景区园林的相关特点 [('特色', 0.3052109479904175), ('苏州园林', 0.29372042417526245), ('江南', 0.2847479581832886), ('风格', 0.2791190445423126), ('岭南', 0.27151352167129517), ('中山', 0.26811009645462036), ('精致', 0.2458246797323227), ('风味', 0.24539321660995483), ('古典', 0.24207080900669098), ('幽静', 0.2402617335319519)]\n",
      "A27景区不错的相关特点 [('活动', 0.24861593544483185), ('观赏', 0.23685996234416962), ('适合', 0.21959275007247925), ('环境保护', 0.21525880694389343), ('游客', 0.20898719131946564), ('欢乐', 0.20702625811100006), ('设备', 0.2068973332643509), ('轻松自在', 0.20409071445465088), ('味道', 0.20405375957489014), ('周边', 0.1957116276025772)]\n",
      "A04景区项目的相关特点 [('比校', 0.4877813756465912), ('避雨', 0.43829745054244995), ('时', 0.4303441345691681), ('开园', 0.419950008392334), ('唱歌', 0.41193607449531555), ('回舍', 0.4073874354362488), ('忘记', 0.4025961458683014), ('台上', 0.39850959181785583), ('没法', 0.3929900825023651), ('玩小', 0.39234602451324463)]\n",
      "A11景区婚纱照的相关特点 [('拍', 0.8900675773620605), ('很想', 0.3572865426540375), ('婚纱', 0.3507867157459259), ('几十对', 0.34653881192207336), ('新人', 0.3421550691127777), ('观景', 0.34030407667160034), ('决心', 0.3399307131767273), ('知觉', 0.33498889207839966), ('虐不轻', 0.3339638113975525), ('影视剧', 0.32671430706977844)]\n",
      "A28景区取票的相关特点 [('快速', 0.817051351070404), ('价格', 0.5541279315948486), ('直得', 0.5011222958564758), ('太高', 0.49541181325912476), ('定位', 0.4787992835044861), ('水浅', 0.43840816617012024), ('友好', 0.4377906024456024), ('锁柜', 0.4305330216884613), ('需', 0.4211655557155609), ('信息', 0.4197351038455963)]\n",
      "A14景区地下河的相关特点 [('灯光效果', 0.39539486169815063), ('千奇百怪', 0.3033924996852875), ('前天', 0.29574054479599), ('好玩', 0.2913673222064972), ('溶洞', 0.2886691689491272), ('想象', 0.2824658453464508), ('沿途', 0.277762770652771), ('旅遊', 0.27466315031051636), ('导游', 0.27341824769973755), ('瀑声', 0.26156798005104065)]\n",
      "A22景区不错的相关特点 [('风景', 0.484956830739975), ('空气', 0.3270258903503418), ('朋友', 0.2961934506893158), ('周末', 0.28947725892066956), ('服务', 0.2886679172515869), ('棒', 0.2744457721710205), ('高', 0.27058181166648865), ('地方', 0.264656662940979), ('设施', 0.25936901569366455), ('游玩', 0.2557297348976135)]\n"
     ]
    }
   ],
   "source": [
    "print(\"A39景区公园的相关特点\",A39)\n",
    "print(\"A38景区温泉的相关特点\",A38)\n",
    "print(\"A23景区园林的相关特点\",A23)\n",
    "print(\"A27景区不错的相关特点\",A27)\n",
    "print(\"A04景区项目的相关特点\",A04)\n",
    "print(\"A11景区婚纱照的相关特点\",A11)\n",
    "print(\"A28景区取票的相关特点\",A28)\n",
    "print(\"A14景区地下河的相关特点\",A14)\n",
    "print(\"A22景区不错的相关特点\",A22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
